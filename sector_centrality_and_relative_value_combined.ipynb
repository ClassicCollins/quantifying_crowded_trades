{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#pip install yahoo_fin\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import yfinance as yf\n",
        "import yahoo_fin.stock_info as si\n",
        "\n",
        "# Calculate Centrality\n",
        "def retrieve_sp500_list():\n",
        "    data = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
        "    table = data[0]\n",
        "    return table\n",
        "\n",
        "def download_sector_data(symbols, start_date, end_date):\n",
        "    df_sectors = yf.download(symbols, start=start_date, end=end_date)['Adj Close']\n",
        "    df_sectors = df_sectors.pct_change().dropna()\n",
        "    return df_sectors\n",
        "\n",
        "def preprocess_data(df_sectors):\n",
        "    scaler = StandardScaler()\n",
        "    df_sectors_scaled = scaler.fit_transform(df_sectors)\n",
        "    return df_sectors_scaled\n",
        "\n",
        "def calculate_weight(market_cap):\n",
        "    weights = [cap / np.sum(market_cap) for cap in market_cap]\n",
        "    return weights\n",
        "\n",
        "def calculate_weighted_df(df_sectors, market_cap):\n",
        "    weights = calculate_weight(market_cap)\n",
        "    sector_weights = dict(zip(df_sectors.columns, [weight * 100 for weight in weights]))\n",
        "\n",
        "    weighted_df = df_sectors.copy()\n",
        "\n",
        "    for sector in df_sectors.columns:\n",
        "        weighted_df.loc[:, sector] *= np.sqrt(sector_weights[sector])\n",
        "\n",
        "    return weighted_df\n",
        "\n",
        "def calculate_centrality_score(X, n=2):\n",
        "    pca_model = PCA(n_components=n)\n",
        "    pca_model.fit(X)\n",
        "    EV = pca_model.components_\n",
        "    AR = pca_model.explained_variance_ratio_\n",
        "\n",
        "    C_list = []\n",
        "    for i in range(X.shape[1]):\n",
        "        C_num = sum(AR[j] * abs(EV[j][i]) / sum(abs(EV[j][k]) for k in range(X.shape[1])) for j in range(n))\n",
        "        C_denom = sum(AR[j] for j in range(n))\n",
        "        C_list.append(C_num / C_denom)\n",
        "\n",
        "    return C_list\n",
        "\n",
        "# Calculate Relative Value\n",
        "\n",
        "def fetch_sector_data(symbols, start_date, end_date):\n",
        "    df_sectors_prices = yf.download(symbols, start=start_date, end=end_date)['Adj Close']\n",
        "    df_sectors = df_sectors_prices.pct_change().dropna()\n",
        "    return df_sectors\n",
        "\n",
        "def calculate_market_cap_weights(market_cap):\n",
        "    weights = market_cap / np.sum(market_cap)\n",
        "    return weights\n",
        "\n",
        "def fetch_sector_current_price(symbols):\n",
        "    sector_current_price = [si.get_live_price(sector) for sector in symbols]\n",
        "    return sector_current_price\n",
        "\n",
        "def fetch_sector_200DaySMA(symbols):\n",
        "    sector_200DaySMA = [si.get_data(sector, interval='1d')['close'][-200:].mean() for sector in symbols]\n",
        "    return sector_200DaySMA\n",
        "\n",
        "def normalize_current_price(current_price, SMA):\n",
        "    normalize = current_price / SMA\n",
        "    return normalize\n",
        "\n",
        "def calculate_relative_measure(normalized_prices, weights):\n",
        "    normalized_prices = np.nan_to_num(normalized_prices, nan=0.0)\n",
        "    weighted_average = np.dot(weights, normalized_prices)\n",
        "    relative_measure = np.zeros_like(normalized_prices)\n",
        "    nonzero_indices = (weighted_average != 0)\n",
        "    relative_measure[nonzero_indices] = normalized_prices[nonzero_indices] / weighted_average[nonzero_indices]\n",
        "    return relative_measure\n",
        "\n",
        "def create_relative_scores_df(symbols, normalized_prices, weights):\n",
        "    relative_measure = calculate_relative_measure(normalized_prices, weights)\n",
        "    relative_scores = dict(zip(symbols, [np.around(r, 2) for r in relative_measure]))\n",
        "    for asset in relative_scores:\n",
        "        relative_scores[asset] = [relative_scores[asset]]\n",
        "    scores_df = pd.DataFrame(data=relative_scores, index=['Relative_score'])\n",
        "    scores_df = scores_df.transpose()\n",
        "    return scores_df\n",
        "\n",
        "def combine_centrality_relative_value(weighted_df, symbols, normalized_prices, weights):\n",
        "    centrality_scores = calculate_centrality_score(weighted_df)\n",
        "    scores_df = create_relative_scores_df(symbols, normalized_prices, weights)\n",
        "\n",
        "    centrality_df = pd.DataFrame(data=centrality_scores, columns=['Centrality'], index=symbols)\n",
        "    relative_df = create_relative_scores_df(symbols, normalized_prices, weights)\n",
        "\n",
        "    combined_scores_df = pd.concat([centrality_df, relative_df], axis=1)\n",
        "    print(combined_scores_df)\n",
        "\n",
        "    combined_scores_df.index.name = 'Ticker'\n",
        "    return combined_scores_df\n",
        "\n",
        "def construct_portfolios(df_sectors, symbols, normalized_prices, weights):\n",
        "    combined_scores_df = combine_centrality_relative_value(weighted_df, symbols, normalized_prices, weights)\n",
        "\n",
        "    try:\n",
        "        top_crowded = combined_scores_df.sort_values(by='Centrality', ascending=False).head(4).index\n",
        "        top_overvalued = combined_scores_df.sort_values(by='Relative_score', ascending=False).head(4).index\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "    if top_crowded.empty or top_overvalued.empty:\n",
        "        print(\"Error: Top crowded or top overvalued is empty.\")\n",
        "        return None, None, None\n",
        "\n",
        "    print(\"Top Four Crowded Sectors:\", top_crowded)\n",
        "    print(\"Top Four Overvalued Sectors:\", top_overvalued)\n",
        "\n",
        "    next_day_returns = df_sectors.shift(-1)\n",
        "\n",
        "    no_bubble_portfolio = combined_scores_df[~combined_scores_df.index.isin(top_crowded.union(top_overvalued))]\n",
        "    bubble_run_up_portfolio = combined_scores_df[combined_scores_df.index.isin(top_crowded) & ~combined_scores_df.index.isin(top_overvalued)]\n",
        "    bubble_sell_off_portfolio = combined_scores_df[combined_scores_df.index.isin(top_crowded) & combined_scores_df.index.isin(top_overvalued)]\n",
        "\n",
        "    return no_bubble_portfolio, bubble_run_up_portfolio, bubble_sell_off_portfolio, next_day_returns\n",
        "\n",
        "# Impute Values below:\n",
        "if __name__ == \"__main__\":\n",
        "    # Input Selected Sectors\n",
        "    symbols = ['XLB', 'XLI', 'XLY', 'XLP', 'XLE', 'XLV', 'XLF', 'XLK', 'XTL', 'XLU', 'XLRE']\n",
        "    # Input Market Capitalization respectively\n",
        "    market_cap = [5150, 37211, 29320, 14101, 52475, 15373, 4368, 13622, 35850, 16512, 48]\n",
        "    # Input the timeframe\n",
        "    start_date = \"2018-01-01\"\n",
        "    end_date = \"2023-01-01\"\n",
        "\n",
        "    sp500_list = retrieve_sp500_list()\n",
        "    sector_data = download_sector_data(symbols, start_date, end_date)\n",
        "    scaled_data = preprocess_data(sector_data)\n",
        "\n",
        "    df_sectors = download_sector_data(symbols, start_date, end_date)\n",
        "\n",
        "    weighted_df = calculate_weighted_df(df_sectors, market_cap)\n",
        "    weighted_scores = calculate_centrality_score(weighted_df)\n",
        "\n",
        "    weights = calculate_market_cap_weights(market_cap)\n",
        "\n",
        "    df_sectors = fetch_sector_data(symbols, start_date, end_date)\n",
        "\n",
        "    sector_current_price = fetch_sector_current_price(symbols)\n",
        "    sector_200DaySMA = fetch_sector_200DaySMA(symbols)\n",
        "\n",
        "    normalized_prices = [normalize_current_price(current_price, sma) for current_price, sma in zip(sector_current_price, sector_200DaySMA)]\n",
        "\n",
        "    relative_measure = calculate_relative_measure(normalized_prices, weights)\n",
        "\n",
        "    scores_df = create_relative_scores_df(symbols, normalized_prices, weights)\n",
        "\n",
        "    df_sectors.index.names = ['Date']\n",
        "\n",
        "    no_bubble_portfolio, bubble_run_up_portfolio, bubble_sell_off_portfolio, next_day_returns = construct_portfolios(df_sectors, symbols, normalized_prices, weights)\n",
        "\n",
        "    print(\"***No Bubble Portfolio***:\")\n",
        "    print(no_bubble_portfolio)\n",
        "    print(\"\\n***Bubble Run-up Portfolio***:\")\n",
        "    print(bubble_run_up_portfolio)\n",
        "    print(\"\\n***Bubble Sell-off Portfolio***:\")\n",
        "    print(bubble_sell_off_portfolio)"
      ],
      "metadata": {
        "id": "FK2DlDEWxi5m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a0016f-9aa2-44e5-f813-3f7be5174aa6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%%**********************]  11 of 11 completed\n",
            "[*********************100%%**********************]  11 of 11 completed\n",
            "[*********************100%%**********************]  11 of 11 completed\n",
            "      Centrality  Relative_score\n",
            "XLB     0.043417            1.01\n",
            "XLI     0.225386            1.03\n",
            "XLY     0.117514            1.05\n",
            "XLP     0.072179            0.96\n",
            "XLE     0.199902            0.99\n",
            "XLV     0.052685            0.99\n",
            "XLF     0.037765            1.05\n",
            "XLK     0.052414            1.12\n",
            "XTL     0.101304            0.94\n",
            "XLU     0.093062            0.96\n",
            "XLRE    0.004372            1.01\n",
            "Top Four Crowded Sectors: Index(['XLI', 'XLE', 'XLY', 'XTL'], dtype='object', name='Ticker')\n",
            "Top Four Overvalued Sectors: Index(['XLK', 'XLY', 'XLF', 'XLI'], dtype='object', name='Ticker')\n",
            "***No Bubble Portfolio***:\n",
            "        Centrality  Relative_score\n",
            "Ticker                            \n",
            "XLB       0.043417            1.01\n",
            "XLP       0.072179            0.96\n",
            "XLV       0.052685            0.99\n",
            "XLU       0.093062            0.96\n",
            "XLRE      0.004372            1.01\n",
            "\n",
            "***Bubble Run-up Portfolio***:\n",
            "        Centrality  Relative_score\n",
            "Ticker                            \n",
            "XLE       0.199902            0.99\n",
            "XTL       0.101304            0.94\n",
            "\n",
            "***Bubble Sell-off Portfolio***:\n",
            "        Centrality  Relative_score\n",
            "Ticker                            \n",
            "XLI       0.225386            1.03\n",
            "XLY       0.117514            1.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xI3H34DKGvkL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}